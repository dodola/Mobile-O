# Mobile-O Android

An Android inference app that runs the Mobile-O multimodal model on-device using ONNX Runtime, supporting text chat, image understanding, and image generation.

---

## Capabilities

| Input | Behavior |
|-------|----------|
| `generate <prompt>` | Text-to-image generation |
| Text + attached image (no prefix) | Image understanding, returns text description |
| Text only | General text chat |

---

## Architecture

```
android/app/src/main/kotlin/com/mobileo/
├── ml/
│   ├── OnnxSessionManager.kt          # Holds all 5 OrtSession instances
│   ├── Tokenizer.kt                   # Qwen2 BPE tokenizer (pure Kotlin)
│   ├── MobileConditioningConnector.kt # LLM hidden states → DiT conditioning
│   ├── MobileOGenerator.kt            # Full text-to-image pipeline
│   ├── DPMSolverScheduler.kt          # DPM-Solver++ scheduler (pure Kotlin)
│   └── InferenceModels.kt             # Image understanding + text chat
├── service/
│   └── ModelDownloadManager.kt        # OkHttp resumable download manager
├── ui/screen/
│   ├── ConversationScreen.kt          # Main chat UI
│   ├── DownloadGateScreen.kt          # First-launch download screen
│   ├── SettingsScreen.kt              # Generation parameter settings
│   └── CameraScreen.kt                # Real-time camera understanding
└── viewmodel/
    ├── ChatViewModel.kt               # Root ViewModel, model loading and routing
    └── SettingsViewModel.kt           # Generation parameters (steps, guidance, etc.)
```

**Runtime dependencies:**
- ONNX Runtime Android 1.20.0 (with NNAPI delegate, automatic CPU fallback)
- NDK NEON kernels for VAE post-processing (`vae_postprocess.cpp`)

---

## Device Requirements

| Requirement | Details |
|-------------|---------|
| Android version | Android 10+ (minSdk 29) |
| CPU architecture | arm64-v8a or x86_64 |
| RAM | 8 GB recommended |
| Storage | ~6 GB free space |

---

## Quick Start (Recommended: Auto-Download)

The app automatically downloads all models from HuggingFace on first launch — no manual setup required.

### Build and Run

```bash
cd android
./gradlew installDebug
```

On first launch:
1. A download screen appears showing the required space (~5 GB)
2. Tap **"Download Models"** — downloads run in the background with resume support
3. After downloading, the app runs fully offline

Models are downloaded from [`Amshaker/Mobile-O-0.5B-Android`](https://huggingface.co/Amshaker/Mobile-O-0.5B-Android) and stored in `filesDir/models/`.

---

## Manual Deployment (Development / Debugging)

If you have already exported models locally using `export_onnx.py`, you can push them directly via `adb` to skip the download screen.

### About the Export Output

`export_onnx.py` automatically handles all Android compatibility concerns — the output is ready to use as-is:

```
onnx_models/
├── llm.onnx              ← graph protobuf (~1.2 MB)
├── llm.onnx.data         ← all weights, contiguous with byte offsets (~2.4 GB)
├── transformer.onnx      ← graph protobuf (~2.1 MB)
├── transformer.onnx.data ← all weights (~2.3 GB)
├── vae_decoder.onnx      ← single file (~608 MB)
├── connector.onnx        ← single file (~9 MB)
├── vision_encoder.onnx   ← single file (~468 MB)
└── llm/                  ← tokenizer files
```

The LLM and DiT models exceed ONNX's 2 GB protobuf size limit, so their weights are stored in `.onnx.data` sidecar files. Android ORT automatically resolves the sidecar when loading the `.onnx` file, as long as both files are in the same directory.

### Step 1: Install the APK

```bash
cd android
./gradlew installDebug
```

### Step 2: Push Model Files

```bash
# Create directories
adb shell mkdir -p /data/data/com.mobileo/files/models/llm

# LLM (graph + sidecar weights)
adb push onnx_models/llm.onnx       /data/data/com.mobileo/files/models/llm.onnx
adb push onnx_models/llm.onnx.data  /data/data/com.mobileo/files/models/llm.onnx.data

# DiT (graph + sidecar weights)
adb push onnx_models/transformer.onnx       /data/data/com.mobileo/files/models/transformer.onnx
adb push onnx_models/transformer.onnx.data  /data/data/com.mobileo/files/models/transformer.onnx.data

# Single-file models
adb push onnx_models/connector.onnx      /data/data/com.mobileo/files/models/connector.onnx
adb push onnx_models/vae_decoder.onnx    /data/data/com.mobileo/files/models/vae_decoder.onnx
adb push onnx_models/vision_encoder.onnx /data/data/com.mobileo/files/models/vision_encoder.onnx

# Tokenizer files
for f in onnx_models/llm/*; do
    adb push "$f" /data/data/com.mobileo/files/models/llm/
done
```

### Step 3: Launch the App

With all model files present, the app skips the download screen on startup and loads models directly.

---

## Model Export

Models are generated by `export_onnx.py` in the parent directory. Quick reference:

```bash
cd Mobile-O-App

# Download checkpoint
python -c "
from huggingface_hub import snapshot_download
snapshot_download(repo_id='Amshaker/Mobile-O-0.5B', local_dir='checkpoints',
                  allow_patterns=['final_merged_model_23620/*'])
"

# Export all components
python export_onnx.py checkpoints --skip-download --output-dir onnx_models
```

Output files:

| File | Size | Description |
|------|------|-------------|
| `llm.onnx` + `llm.onnx.data` | ~2.4 GB | Qwen2-0.5B with all 25 hidden states |
| `transformer.onnx` + `transformer.onnx.data` | ~2.3 GB | SANA DiT |
| `vae_decoder.onnx` | ~608 MB | DC-AE VAE decoder |
| `connector.onnx` | ~9 MB | MobileConditioningProjector |
| `vision_encoder.onnx` | ~468 MB | MobileCLIP vision encoder |
| `llm/` | < 1 MB | Qwen2 BPE tokenizer files |

The exporter automatically:
- Consolidates scattered per-tensor weight files into a single `.onnx.data` sidecar (fixes Android file-descriptor limits)
- Repairs invalid `-1` values in VAE Transpose `perm` attributes (fixes ORT TypeInferenceError)

---

## Inference Pipelines

### Text-to-Image

```
User prompt (generate <prompt>)
    │
    ▼
Tokenizer.encodeGenerationPrompt()      # ChatML formatting + BPE encoding
    │
    ▼
llm.onnx (LLMWrapper)                   # inputs:  [input_ids, attention_mask]
    │                                   # outputs: logits + hidden_state_0..24
    ▼
MobileConditioningConnector             # stacks last 4 hidden layers
    │   connector.onnx                  # output: [1, seqLen, 2304] conditioning
    ▼
DPMSolverScheduler                      # builds noise timestep sequence
    │
    ▼
denoising loop (default 15 steps)
    │   transformer.onnx (DiTWrapper)   # inputs:  latent, timestep, conditioning
    │                                   # output:  noise_pred
    │   CFG: noise = uncond + scale * (cond - uncond)
    ▼
vae_decoder.onnx (VAEDecoderWrapper)    # input:  latent [1, 32, 16, 16]
    │                                   # output: image  [1,  3, 512, 512]
    ▼
VaePostProcessor.floatCHWtoBitmap()     # NDK NEON: [-1,1] → ARGB_8888 Bitmap
```

### Image Understanding

```
User image + text question
    │
    ▼
ImageProcessor                          # resize → 1024×1024 → CHW float [0,1]
    │
    ▼
vision_encoder.onnx (MobileCLIPVisionTower)
    │                                   # output: [1, 256, 3072] patch features
    ▼
llm.onnx                                # interleaved image+text tokens, greedy decode
    │                                   # up to 256 new tokens
    ▼
Tokenizer.decode()                      # returns text answer
```

---

## Generation Settings

Adjustable in SettingsScreen:

| Parameter | Default | Range | Description |
|-----------|---------|-------|-------------|
| Steps | 15 | 5–50 | More steps = higher quality, slower |
| Guidance scale | 1.3 | 1.0–10.0 | CFG scale — higher values follow the prompt more closely |
| Enable CFG | On | On/Off | Disabling CFG roughly doubles generation speed |
| Seed | Random | Any integer | Fixed seed produces reproducible results |

---

## Troubleshooting

### ORT_INVALID_PROTOBUF

```
Error code - ORT_INVALID_PROTOBUF: Load model from .../llm.onnx failed:
Protobuf parsing failed.
```

**Cause:** The `.onnx.data` sidecar file is missing, or `.onnx` and `.onnx.data` are not in the same directory.

**Fix:** Make sure both files are pushed together:
```bash
adb push onnx_models/llm.onnx      /data/data/com.mobileo/files/models/llm.onnx
adb push onnx_models/llm.onnx.data /data/data/com.mobileo/files/models/llm.onnx.data
```

### App Crashes on Model Load (OOM)

The LLM alone occupies ~2.4 GB of RAM; all models together require ~5–6 GB. Devices with less than 8 GB RAM may trigger the OOM Killer. Run on a device with 8 GB RAM or more.

### NNAPI Errors But App Still Works

`OnnxSessionManager` automatically falls back to CPU if NNAPI is unavailable, logging a warning. This is expected on devices without an NPU/DSP and does not affect functionality.

---

## Known Limitations

- Model loading takes 10–30 seconds depending on device storage speed
- Image generation takes 30–120 seconds depending on device CPU/NPU
- NNAPI acceleration requires device support; CPU fallback is automatic
- Image editing (edit mode) is not implemented in the Android version — iOS only
